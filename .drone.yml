kind: pipeline
type: docker
name: Build Llama Server Binaries

steps:
- name: Get llama.cpp
  image: drone/git
  commands:
  - mkdir builds
  - git clone https://github.com/ggerganov/llama.cpp
  - cd llama.cpp
  - |
    if [ -n "$VERSION" ];
    then export LLAMA_CPP_VERSION=$VERSION;
    else export LLAMA_CPP_VERSION=$(git describe --tags --abbrev=0);
    fi
  - git checkout ${LLAMA_CPP_VERSION}
  - echo "Building llama.cpp Version $LLAMA_CPP_VERSION"

- name: Build CPU
  image: ubuntu:24.04
  depends_on:
  - Get llama.cpp
  commands:
  - apt-get update && apt-get install -y git curl wget cmake build-essential
  - cp -r /drone/src/llama.cpp /app && cd /app
  - cmake -B build -S . -DBUILD_SHARED_LIBS=OFF -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined
  - cmake --build build --config Release --target llama-server -j $(nproc)
  - cd ./build/bin/
  - tar -acf /drone/src/builds/llama-server-cpu.tar.gz llama-server
  
- name: Build Vulkan
  image: ubuntu:24.04
  depends_on:
  - Get llama.cpp
  commands:
  - apt-get update && apt-get install -y git curl wget cmake build-essential libcurl4-openssl-dev
  - wget -qO- https://packages.lunarg.com/lunarg-signing-key-pub.asc | tee /etc/apt/trusted.gpg.d/lunarg.asc
  - wget -qO /etc/apt/sources.list.d/lunarg-vulkan-noble.list http://packages.lunarg.com/vulkan/lunarg-vulkan-noble.list
  - apt-get update && apt-get install -y vulkan-sdk
  - cp -r /drone/src/llama.cpp /app && cd /app
  - cmake -B build -S . -DGGML_VULKAN=1 -DBUILD_SHARED_LIBS=OFF -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined
  - cmake --build build --config Release --target llama-server -j $(nproc)
  - cd ./build/bin/
  - tar -acf /drone/src/builds/llama-server-vulkan.tar.gz ./llama-server

- name: Build Cuda
  image: nvidia/cuda:12.6.0-devel-ubuntu24.04
  depends_on:
  - Get llama.cpp
  commands:
  - apt-get update && apt-get install -y git curl wget cmake build-essential
  # -DCMAKE_CUDA_ARCHITECTURES=61
  - cp -r /drone/src/llama.cpp /app && cd /app
  - cmake -B build -S . -DGGML_CUDA=ON -DBUILD_SHARED_LIBS=OFF -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined
  - cmake --build build --config Release --target llama-server -j $(nproc)
  - cd ./build/bin/
  - tar -acf /drone/src/builds/llama-server-cuda.tar.gz ./llama-server

# - name: Build Rocm
#   image: rocm/dev-ubuntu-24.04:6.2-complete
#   depends_on:
#   - Get llama.cpp
#   commands:
#   - cd llama.cpp
#   - apt-get update && apt-get install -y git curl wget cmake build-essential
#   # - export GPU_TARGETS=gfx803 gfx900 gfx906 gfx908 gfx90a gfx1010 gfx1030 gfx1100 gfx1101 gfx1102
#   - export HIPCXX="$(hipconfig -l)/clang"
#   - export HIP_PATH="$(hipconfig -R)" 
#   - cmake -B build-rocm -S . -DGGML_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release -DBUILD_SHARED_LIBS=OFF -DCMAKE_EXE_LINKER_FLAGS=-Wl,--allow-shlib-undefined -DCMAKE_C_FLAGS="-fPIE" -DCMAKE_CXX_FLAGS="-fPIE"
#   - cmake --build build-rocm --config Release --target llama-server -j $(nproc)
#   - cd ./build-rocm/bin
#   - tar -acf /drone/src/builds/llama-server-rocm.tar.gz llama-server

- name: Upload to Github
  image: ghcr.io/supportpal/github-gh-cli
  depends_on:
  - Build CPU
  - Build Vulkan
  - Build Cuda
  environment:
    GITHUB_TOKEN:
      from_secret: GITHUB_TOKEN
  entrypoint: ["/bin/bash"]
  commands:
  # Get the version from the llama.cpp repository
  - cd /drone/src/llama.cpp && export LLAMA_CPP_VERSION=$(git describe --tags --abbrev=0);
  - echo "Uploading to Github release $LLAMA_CPP_VERSION"
  - cd /drone/src/builds && ls -la
  - |
    if gh release view $LLAMA_CPP_VERSION;
    then echo "Release ${LLAMA_CPP_VERSION} already exists";
    else gh release create $LLAMA_CPP_VERSION -t $LLAMA_CPP_VERSION -n "Automated release";
    fi
  - gh release upload --clobber $LLAMA_CPP_VERSION ./*.tar.gz
